---
layout: default
title: Home
---

<head>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>


<div class="page">
  
  <header>
    <h1 class="landing-title">Violin Diffusion</h1>
    <p style="font-size: 1.25em; font-style: italic; text-align: center; color: #555;">
      <a href="https://scholar.google.com/citations?user=qzvPTyAAAAAJ&hl=ko" target="_blank" style="color: #1a73e8;  text-decoration: none;">Daewoong Kim</a>, 
      <a href="https://scholar.google.com/citations?user=tEOa3O4AAAAJ" target="_blank" style="color: #1a73e8;  text-decoration: none;">Hao-Wen Dong</a>, 
      and 
      <a href="https://scholar.google.com/citations?hl=ko&user=yM24uskAAAAJ" target="_blank" style="color: #1a73e8;  text-decoration: none;">Dasaem Jeong</a>
    </p>
    

    <div class="social-icons text-center">
      <a href="https://arxiv.org/abs/your-paper-link" target="_blank">
        <i class="fa fa-file-pdf-o fa-2x" aria-hidden="true" title="View Paper on arXiv"></i>
      </a>
      <!-- <a href="https://github.com/your-repo-link" target="_blank">
        <i class="fa fa-github fa-2x" aria-hidden="true" title="View Code on GitHub"></i>
      </a> -->
      <a href="{{ site.baseurl }}/listening_tests/" target="_blank">
        <i class="fa fa-music fa-2x" aria-hidden="true" title="Audio Examples"></i>
      </a>
    </div>

  </header>



  <section class="abstract-section">
    <h2>Abstract</h2>
    <p class="message">
      Synthesizing expressive violin performances is a complex task due to the need for intricate pitch variations, such as vibrato and glissando, which are often missing in standard MIDI data. To address this, we introduce a two-stage neural synthesis framework. The first stage, DiffBend, estimates the missing pitch bend information, while the second stage, DiffSynth, generates high-fidelity audio incorporating these expressive details. Our method is evaluated using Fr√©chet Audio Distance (FAD) and vibrato-specific metrics, demonstrating superior performance in both audio quality and expressiveness compared to existing methods. This highlights the critical role of accurate pitch bend modeling in improving the realism of synthesized violin performances.    
    </p>
  </section>

  <section class="midi-encoding-section">
    <h2>Bend Roll</h2>

    <p class="message">
      To encode MIDI input, we employed the piano roll representation, which facilitates encoding polyphonic pitches while aligning them with the mel spectrogram at the frame level.To capture the onset and offset of each note alongside its frame-wise activation, we utilize three types of piano rolls: frame, onset, offset.
      In addition to these binary piano rolls, pitch bend information is also included in the model's input. In MIDI, pitch bends are discrete events that specify a pitch bend value and its corresponding time, enabling smooth pitch changes. However, these events often occur more frequently than traditional piano roll representations can capture. Thus, we calculate frame-wise pitch bend values using a weighted average of pitch bend events within each frame, resulting in a continuous-valued matrix.   
      The encoded result \( R_{\text{bend}} \in (-1, 1)^{P \times T} \) is computed using the equation below:
      </p>
      <p class="equation">
        The encoded result $R_{\text{bend}} \in (-1, 1)^{P \times T}$ is computed using the equation below:
      $$ R_\text{bend}(p, n) = \frac{\sum_{i} b_i^p \cdot d_i^p}{\sum_{i} d_i^p} $$
      Here, $b_i^p$ represents the $i$-th pitch bend value (in semitones) for pitch $p$, and $d_i^p$ is the duration for which that event is active within the time frame $n$. The duration of a pitch bend event is defined as the time it remains active until the next event.
      </p>
    <p class="message">
      This method is versatile and can be applied to both monophonic and polyphonic music, where multiple pitches may undergo different pitch bends simultaneously.
    </p>


    </section>

    <section class="midi-encoding-section">
      <h2>MIDI Encoding</h2>
      <img src="images/bendroll_1.png" alt="Bend Roll Example 1" style="width: 100%; max-width: 600px; margin-top: 20px;">
      <img src="images/bendroll_2.png" alt="Bend Roll Example 2" style="width: 100%; max-width: 600px; margin-top: 20px;">

      <p class="message">
        To encode MIDI input, we employed piano roll representation as it enables encoding polyphonic pitch while being aligned to mel spectrogram at the frame level. If a mel spectrogram \( M \in \mathbb{R}^{F\times T} \) consists of \( T \) time frames and \( F \) mel bins, the input piano roll is a 2D binary matrix \( R \in \{0, 1\}^{P \times T} \), where \( P \) denotes the number of pitches. To emphasize the importance of the onset and offset of each note along with its frame-wise activation, we employed three different types of rolls: \( R_{\text{frame}}, R_{\text{onset}}, R_{\text{offset}} \).
        <br><br>
        In addition to these multiple binary piano rolls, we also include pitch bend information in the model's input. Pitch bends in MIDI are discrete events, each with a pitch bend value and a corresponding time, which enable the continuous change of pitch. These events often occur at finer intervals than what traditional piano roll representations can capture. Therefore, we calculate frame-wise pitch bend value by weighted average considering the duration of pitch bend events within each frame, which results in \( R_{\text{bend}} \in (-1, 1)^{P \times T} \).
        <br><br>
        The encoded result \( R_{\text{bend}} \in (-1, 1)^{P \times T} \) is computed using the equation below:
        <br><br>
        $$ R_\text{bend}(p, n) = \frac{\sum_{i} b_i^p \cdot d_i^p}{\sum_{i} d_i^p} $$
        <br><br>
        Here, \( b_i^p \) is the \( i \)-th pitch bend value (in semitones) for pitch \( p \), and \( d_i^p \) is the duration for which that event is active within time frame \( n \). The duration of a pitch bend event is defined as the time it remains active until the next event. This method generalizes well to both monophonic and polyphonic music, where multiple pitches may undergo different pitch bends simultaneously.
      </p>
    </section>
    
  <section class="model-architecture-section">
    <h2>Model Architecture</h2>
    <img src="images/model_architecture.jpeg" alt="Model Architecture Diagram" style="width: 100%; max-width: 600px; margin-top: 20px;">
    <img src="images/denoiser.jpeg" alt="Model Architecture Diagram" style="width: 100%; max-width: 600px; margin-top: 20px;">

    
  <!-- Synthesis Module -->
  <div class="subsection">
    <h3>Synthesis Module</h3>
    <div class="message">
      In the synthesis module, the encoder processes the rolls \( \mathcal{R} \) as input through a GRU layer, followed by Transformer Encoder Blocks, producing the note feature sequence \( E_R \). A simple linear layer then generates an auxiliary mel spectrogram \( \tilde{M} \) from \( E_R \). For the denoiser of the synthesis module, we adopt the non-causal WaveNet architecture with additional modifications to incorporate performer conditioning through FiLM layers.
      <br><br>
      To denoise the noisy mel spectrogram \( M_t \in \mathbb{R}^{F\times T} \), we first add a time step embedding \( e_t \in \mathbb{R}^D \) by passing it through a linear layer \( f: \mathbb{R}^D \rightarrow \mathbb{R}^F \) to each time frame of \( M_t \). The combined input is modulated again via a FiLM layer, using the auxiliary mel spectrogram \( \tilde{M} \) as a conditioning input.
      <br><br>
      Each residual block in the denoiser receives conditioning inputs \( e_t \), \( e_p \), and \( E_R \). The performer embedding \( e_p \) is processed through FiLM layers to modulate the internal processing within the residual block, while the time step embedding \( e_t \) and the note features \( E_R \) are added directly.
      <br><br>
      The model is trained to minimize the following loss function:
      \[
      \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{simple}} + \mathcal{L}_{\text{aux}}
      \]
      where the \( \mathcal{L}_{\text{simple}} \) loss follows the formulation used to predict the noise \( \epsilon \) added to the noisy mel spectrogram \( M_t \).
      \[
      \mathcal{L}_{\text{simple}} = \mathbb{E}_{M_0, \epsilon} \left[ \left\| \epsilon - \epsilon_\theta \left( \sqrt{\bar{\alpha}_t} M_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t, c \right) \right\|^2 \right].
      \]
      In addition, the auxiliary loss \( \mathcal{L}_{\text{aux}} \) is used to minimize the L2 reconstruction error between the predicted auxiliary mel spectrogram \( \tilde{M} \) and the ground truth mel spectrogram \( M \).
    </div>

  </div>

  <!-- Bend Estimation Module -->
  <div class="subsection">
    <h3>Bend Estimation Module</h3>
    <div class="message">
      The bend estimation module is adapted from the synthesis module to handle pitch bend data directly. Unlike the synthesis module, which conditions its denoising process on an auxiliary mel spectrogram \( \tilde{M} \), the bend estimation module operates directly on the noisy bend roll \( R_{\text{bend}_t} \).
      During the diffusion, we only add noise to the regions with note activation. Other regions were excluded in noise addition and loss calculation.
    </div>
  </div>

</section>

    <p class="message">
      The DiffSynth model processes inputs including the Mel-spectrogram, diffusion time step, performer ID, and piano rolls, using a GRU layer and Transformer Encoder Blocks to generate a feature sequence for denoising. DiffBend modifies this by directly handling the pitch bend roll and applying a selective noise addition strategy to focus on dynamic pitch variations. These models together enhance the expressiveness of synthesized violin performances by effectively incorporating detailed pitch variations.
    </p>
  </section>


<!--   
  <div class="text-center">
    <p>
      <i class="fa fa-code-fork fa-3x"></i>
    </p>

    <p>
      Welcome to the Violin Diffusion project! This research focuses on synthesizing expressive violin performances through advanced neural synthesis techniques.
    </p>
    
    <h2>Abstract</h2>
    <p>
      Synthesizing expressive violin performances is a complex task due to the need for intricate pitch variations, such as vibrato and glissando, which are often missing in standard MIDI data. To address this, we introduce a two-stage neural synthesis framework. The first stage, <strong>DiffBend</strong>, estimates the missing pitch bend information, while the second stage, <strong>DiffSynth</strong>, generates high-fidelity audio incorporating these expressive details. Our method is evaluated using Fr√©chet Audio Distance (FAD) and vibrato-specific metrics, demonstrating superior performance in both audio quality and expressiveness compared to existing methods.
    </p>

    <p>
      This is your landing page. It is the first page your users will see when 
      they visit your website. Perhaps write a little bit about you or your 
      website here.
    </p>

    <p>
      Feel free to dive right in and make your changes to the code!<br />
      Or check out Celeste on <a href="https://github.com/nicoelayda/celeste">GitHub</a> 
      to learn more.
    </p>
  </div> -->
  
</div>
