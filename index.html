---
layout: default
title: Home
---
<div class="page">
  
  <header>
    <h1 class="landing-title">Violin Diffusion</h1>
    <p style="font-size: 1.25em; font-style: italic; text-align: center; color: #555;">
      <a href="https://scholar.google.com/citations?user=qzvPTyAAAAAJ&hl=ko" target="_blank" style="color: #1a73e8;  text-decoration: none;">Daewoong Kim</a>, 
      <a href="https://scholar.google.com/citations?user=tEOa3O4AAAAJ" target="_blank" style="color: #1a73e8;  text-decoration: none;">Hao-Wen Dong</a>, 
      and 
      <a href="https://scholar.google.com/citations?hl=ko&user=yM24uskAAAAJ" target="_blank" style="color: #1a73e8;  text-decoration: none;">Dasaem Jeong</a>
    </p>
    

    <div class="social-icons text-center">
      <a href="https://arxiv.org/abs/your-paper-link" target="_blank">
        <i class="fa fa-file-pdf-o fa-2x" aria-hidden="true" title="View Paper on arXiv"></i>
      </a>
      <!-- <a href="https://github.com/your-repo-link" target="_blank">
        <i class="fa fa-github fa-2x" aria-hidden="true" title="View Code on GitHub"></i>
      </a> -->
      <a href="{{ site.baseurl }}/listening_tests/" target="_blank">
        <i class="fa fa-music fa-2x" aria-hidden="true" title="Audio Examples"></i>
      </a>
    </div>

  </header>




  <section class="abstract-section">
    <h2>Abstract</h2>
    <p class="message">
      Synthesizing expressive violin performances is a complex task due to the need for intricate pitch variations, such as vibrato and glissando, which are often missing in standard MIDI data. To address this, we introduce a two-stage neural synthesis framework. The first stage, DiffBend, estimates the missing pitch bend information, while the second stage, DiffSynth, generates high-fidelity audio incorporating these expressive details. Our method is evaluated using Fréchet Audio Distance (FAD) and vibrato-specific metrics, demonstrating superior performance in both audio quality and expressiveness compared to existing methods. This highlights the critical role of accurate pitch bend modeling in improving the realism of synthesized violin performances.    
    </p>
  </section>

  <section class="bend-roll-section">
    <h2>Bend Roll</h2>
    <img src="images/bendroll1.png" alt="Bend Roll Example 1" style="width: 100%; max-width: 600px; margin-top: 20px;">
    <img src="images/bendroll2.png" alt="Bend Roll Example 2" style="width: 100%; max-width: 600px; margin-top: 20px;">
    <p class="message">
      Pitch bends in MIDI data present a unique challenge due to their fine temporal granularity, which is not well captured by traditional fixed-time piano roll representations. To address this, we aggregate all pitch bend information that falls within each fixed time frame of the piano roll. Each pitch bend value, \( b_i \), is converted to semitone units and weighted by its corresponding duration \( d_i \) within that time frame. The aggregated bend roll is then computed for the entire sequence as shown in the equation below:
      This weighted averaging process allows us to accurately capture the pitch bend information within the fixed time bins of the piano roll, ensuring that the expressive nuances of the bends are well-preserved. These bend roll features are crucial for modeling pitch variations in both the DiffSynth and DiffBend architectures.
    </p>
  </section>

  <section class="model-architecture-section">
    <h2>Model Architecture</h2>
    <img src="images/model_architecture.jpeg" alt="Model Architecture Diagram" style="width: 100%; max-width: 600px; margin-top: 20px;">
    <p class="message">
      The DiffSynth model processes inputs including the Mel-spectrogram, diffusion time step, performer ID, and piano rolls, using a GRU layer and Transformer Encoder Blocks to generate a feature sequence for denoising. DiffBend modifies this by directly handling the pitch bend roll and applying a selective noise addition strategy to focus on dynamic pitch variations. These models together enhance the expressiveness of synthesized violin performances by effectively incorporating detailed pitch variations.
    </p>
  </section>


<!--   
  <div class="text-center">
    <p>
      <i class="fa fa-code-fork fa-3x"></i>
    </p>

    <p>
      Welcome to the Violin Diffusion project! This research focuses on synthesizing expressive violin performances through advanced neural synthesis techniques.
    </p>
    
    <h2>Abstract</h2>
    <p>
      Synthesizing expressive violin performances is a complex task due to the need for intricate pitch variations, such as vibrato and glissando, which are often missing in standard MIDI data. To address this, we introduce a two-stage neural synthesis framework. The first stage, <strong>DiffBend</strong>, estimates the missing pitch bend information, while the second stage, <strong>DiffSynth</strong>, generates high-fidelity audio incorporating these expressive details. Our method is evaluated using Fréchet Audio Distance (FAD) and vibrato-specific metrics, demonstrating superior performance in both audio quality and expressiveness compared to existing methods.
    </p>

    <p>
      This is your landing page. It is the first page your users will see when 
      they visit your website. Perhaps write a little bit about you or your 
      website here.
    </p>

    <p>
      Feel free to dive right in and make your changes to the code!<br />
      Or check out Celeste on <a href="https://github.com/nicoelayda/celeste">GitHub</a> 
      to learn more.
    </p>
  </div> -->
  
</div>
